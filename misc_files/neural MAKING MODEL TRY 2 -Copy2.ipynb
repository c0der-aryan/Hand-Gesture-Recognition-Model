{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea9c4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn pandas optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ca4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/aryansood/aims/AIMS_DRONE2/newdata_final_with_updown_flip.csv\")\n",
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "X = df_shuffled.drop('label', axis=1)  # Features\n",
    "y = df_shuffled['label']  # Labels\n",
    "\n",
    "# Split the shuffled data into 80% training, 10% validation, and 10% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a518927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "44/44 [==============================] - 1s 4ms/step - loss: 50.1303 - accuracy: 0.3132 - val_loss: 1.2268 - val_accuracy: 0.7931\n",
      "Epoch 2/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 2.1029 - accuracy: 0.5596 - val_loss: 0.6525 - val_accuracy: 0.7529\n",
      "Epoch 3/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 1.1167 - accuracy: 0.6013 - val_loss: 0.5122 - val_accuracy: 0.8161\n",
      "Epoch 4/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9779 - accuracy: 0.6401 - val_loss: 0.4570 - val_accuracy: 0.8218\n",
      "Epoch 5/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.8691 - accuracy: 0.6588 - val_loss: 0.4413 - val_accuracy: 0.8046\n",
      "Epoch 6/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.8915 - accuracy: 0.6401 - val_loss: 0.4291 - val_accuracy: 0.8563\n",
      "Epoch 7/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 1.0504 - accuracy: 0.6264 - val_loss: 0.4791 - val_accuracy: 0.8103\n",
      "Epoch 8/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9709 - accuracy: 0.6272 - val_loss: 0.4434 - val_accuracy: 0.8506\n",
      "Epoch 9/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9962 - accuracy: 0.6135 - val_loss: 0.5998 - val_accuracy: 0.6954\n",
      "Epoch 10/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9463 - accuracy: 0.6257 - val_loss: 0.4478 - val_accuracy: 0.8506\n",
      "Epoch 11/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9400 - accuracy: 0.6379 - val_loss: 0.4705 - val_accuracy: 0.8161\n",
      "Epoch 12/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.8379 - accuracy: 0.6753 - val_loss: 0.3946 - val_accuracy: 0.8391\n",
      "Epoch 13/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.8127 - accuracy: 0.6753 - val_loss: 0.3213 - val_accuracy: 0.8161\n",
      "Epoch 14/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7946 - accuracy: 0.6990 - val_loss: 0.3868 - val_accuracy: 0.8391\n",
      "Epoch 15/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.8817 - accuracy: 0.6616 - val_loss: 0.2974 - val_accuracy: 0.8908\n",
      "Epoch 16/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7632 - accuracy: 0.6832 - val_loss: 0.2734 - val_accuracy: 0.8678\n",
      "Epoch 17/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7584 - accuracy: 0.6868 - val_loss: 0.2412 - val_accuracy: 0.8793\n",
      "Epoch 18/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9037 - accuracy: 0.6394 - val_loss: 0.3269 - val_accuracy: 0.8506\n",
      "Epoch 19/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.8041 - accuracy: 0.6875 - val_loss: 0.3231 - val_accuracy: 0.8678\n",
      "Epoch 20/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7450 - accuracy: 0.7011 - val_loss: 0.2630 - val_accuracy: 0.8506\n",
      "Epoch 21/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7855 - accuracy: 0.7019 - val_loss: 0.3577 - val_accuracy: 0.8621\n",
      "Epoch 22/150\n",
      " 1/44 [..............................] - ETA: 0s - loss: 0.6612 - accuracy: 0.7500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# preferred_model.compile(\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     optimizer=tf.keras.optimizers.Adam(0.001),\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_encoded, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val_encoded))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    870\u001b[0m   )\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1492\u001b[0m   )\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test' are your datasets\n",
    "num_classes=8\n",
    "\n",
    "# Encode labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(X_train.shape[1:])),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# preferred_model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\n",
    "#     optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "# )\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded, epochs=150, validation_data=(X_val, y_val_encoded))\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b775b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7af147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "test_accuracy,test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d201851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_new.pkl\n",
    "# neural_new1.pkl\n",
    "# neural MAKING MODEL TRY 2 -Copy2.ipynb\n",
    "# ['neural_new2.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf65d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "44/44 [==============================] - 1s 4ms/step - loss: 22.0579 - accuracy: 0.3764 - val_loss: 1.4017 - val_accuracy: 0.6676\n",
      "Epoch 2/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7943 - accuracy: 0.8003 - val_loss: 0.4029 - val_accuracy: 0.8338\n",
      "Epoch 3/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.8254 - val_loss: 0.6010 - val_accuracy: 0.8252\n",
      "Epoch 4/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.6997 - accuracy: 0.8470 - val_loss: 0.5176 - val_accuracy: 0.8338\n",
      "Epoch 5/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8743 - val_loss: 0.5669 - val_accuracy: 0.8596\n",
      "Epoch 6/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.8779 - val_loss: 1.0789 - val_accuracy: 0.8539\n",
      "Epoch 7/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.5280 - accuracy: 0.8671 - val_loss: 0.3052 - val_accuracy: 0.8510\n",
      "Epoch 8/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.8599 - val_loss: 0.2555 - val_accuracy: 0.8711\n",
      "Epoch 9/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.8563 - val_loss: 0.4307 - val_accuracy: 0.8653\n",
      "Epoch 10/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8786 - val_loss: 0.4249 - val_accuracy: 0.8739\n",
      "Epoch 11/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8750 - val_loss: 0.2541 - val_accuracy: 0.8940\n",
      "Epoch 12/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8700 - val_loss: 0.3687 - val_accuracy: 0.8711\n",
      "Epoch 13/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.8664 - val_loss: 0.3312 - val_accuracy: 0.8797\n",
      "Epoch 14/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2598 - accuracy: 0.8843 - val_loss: 0.2529 - val_accuracy: 0.8481\n",
      "Epoch 15/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.8786 - val_loss: 0.2955 - val_accuracy: 0.8510\n",
      "Epoch 16/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.8843 - val_loss: 0.2116 - val_accuracy: 0.8510\n",
      "Epoch 17/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2834 - accuracy: 0.8858 - val_loss: 0.2617 - val_accuracy: 0.8539\n",
      "Epoch 18/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.8822 - val_loss: 0.2651 - val_accuracy: 0.8625\n",
      "Epoch 19/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2883 - accuracy: 0.8779 - val_loss: 0.2586 - val_accuracy: 0.8711\n",
      "Epoch 20/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.8477 - accuracy: 0.8089 - val_loss: 0.6024 - val_accuracy: 0.8395\n",
      "Epoch 21/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4936 - accuracy: 0.8534 - val_loss: 0.2700 - val_accuracy: 0.8711\n",
      "Epoch 22/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.8822 - val_loss: 0.4081 - val_accuracy: 0.8567\n",
      "Epoch 23/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.8886 - val_loss: 0.2151 - val_accuracy: 0.8825\n",
      "Epoch 24/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2595 - accuracy: 0.8728 - val_loss: 0.2417 - val_accuracy: 0.8854\n",
      "Epoch 25/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.8872 - val_loss: 0.2251 - val_accuracy: 0.8481\n",
      "Epoch 26/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.8922 - val_loss: 0.3641 - val_accuracy: 0.8739\n",
      "Epoch 27/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.8815 - val_loss: 0.1912 - val_accuracy: 0.8653\n",
      "Epoch 28/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.8901 - val_loss: 0.2104 - val_accuracy: 0.8625\n",
      "Epoch 29/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.8937 - val_loss: 0.2465 - val_accuracy: 0.8567\n",
      "Epoch 30/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2013 - accuracy: 0.8894 - val_loss: 0.1915 - val_accuracy: 0.8567\n",
      "Epoch 31/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.8793 - val_loss: 0.3099 - val_accuracy: 0.8625\n",
      "Epoch 32/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.8901 - val_loss: 0.2140 - val_accuracy: 0.8911\n",
      "Epoch 33/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.8951 - val_loss: 0.3066 - val_accuracy: 0.8625\n",
      "Epoch 34/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.8836 - val_loss: 0.2062 - val_accuracy: 0.8567\n",
      "Epoch 35/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.8750 - val_loss: 0.2132 - val_accuracy: 0.8567\n",
      "Epoch 36/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.8908 - val_loss: 0.1998 - val_accuracy: 0.8596\n",
      "Epoch 37/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8606 - val_loss: 0.5684 - val_accuracy: 0.7736\n",
      "Epoch 38/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.8671 - val_loss: 0.2246 - val_accuracy: 0.8682\n",
      "Epoch 39/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2773 - accuracy: 0.8736 - val_loss: 0.2260 - val_accuracy: 0.8596\n",
      "Epoch 40/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.8736 - val_loss: 0.2764 - val_accuracy: 0.8539\n",
      "Epoch 41/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.8721 - val_loss: 0.1879 - val_accuracy: 0.8997\n",
      "Epoch 42/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.8764 - val_loss: 0.1996 - val_accuracy: 0.8911\n",
      "Epoch 43/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.8772 - val_loss: 0.2016 - val_accuracy: 0.8510\n",
      "Epoch 44/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.8836 - val_loss: 0.2286 - val_accuracy: 0.8854\n",
      "Epoch 45/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.8872 - val_loss: 0.2079 - val_accuracy: 0.8510\n",
      "Epoch 46/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.8815 - val_loss: 0.2224 - val_accuracy: 0.8625\n",
      "Epoch 47/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.8750 - val_loss: 0.1894 - val_accuracy: 0.8539\n",
      "Epoch 48/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.8822 - val_loss: 0.1888 - val_accuracy: 0.8539\n",
      "Epoch 49/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.8922 - val_loss: 0.1902 - val_accuracy: 0.8567\n",
      "Epoch 50/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.8872 - val_loss: 0.1800 - val_accuracy: 0.8653\n",
      "Epoch 51/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.8829 - val_loss: 0.1901 - val_accuracy: 0.8625\n",
      "Epoch 52/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.8786 - val_loss: 0.1784 - val_accuracy: 0.8653\n",
      "Epoch 53/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.8901 - val_loss: 0.2176 - val_accuracy: 0.8625\n",
      "Epoch 54/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.8915 - val_loss: 0.1847 - val_accuracy: 0.8510\n",
      "Epoch 55/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.8894 - val_loss: 0.1831 - val_accuracy: 0.8911\n",
      "Epoch 56/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.8757 - val_loss: 0.1996 - val_accuracy: 0.8481\n",
      "Epoch 57/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.8851 - val_loss: 0.1839 - val_accuracy: 0.8883\n",
      "Epoch 58/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.8858 - val_loss: 0.2101 - val_accuracy: 0.8567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.8922 - val_loss: 0.1945 - val_accuracy: 0.8682\n",
      "Epoch 60/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.8851 - val_loss: 0.1826 - val_accuracy: 0.8596\n",
      "Epoch 61/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.8915 - val_loss: 0.1822 - val_accuracy: 0.8825\n",
      "Epoch 62/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1659 - accuracy: 0.8930 - val_loss: 0.1880 - val_accuracy: 0.8510\n",
      "Epoch 63/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.8858 - val_loss: 0.1822 - val_accuracy: 0.8739\n",
      "Epoch 64/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.8786 - val_loss: 0.2219 - val_accuracy: 0.8625\n",
      "Epoch 65/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.8951 - val_loss: 0.1853 - val_accuracy: 0.8625\n",
      "Epoch 66/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.8800 - val_loss: 0.1784 - val_accuracy: 0.8911\n",
      "Epoch 67/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.8815 - val_loss: 0.1797 - val_accuracy: 0.8510\n",
      "Epoch 68/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.8951 - val_loss: 0.2574 - val_accuracy: 0.8510\n",
      "Epoch 69/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.8851 - val_loss: 0.1869 - val_accuracy: 0.8968\n",
      "Epoch 70/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.8412 - val_loss: 0.2677 - val_accuracy: 0.8625\n",
      "Epoch 71/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.8966 - accuracy: 0.7945 - val_loss: 3.9346 - val_accuracy: 0.6189\n",
      "Epoch 72/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9128 - accuracy: 0.7909 - val_loss: 0.3768 - val_accuracy: 0.8195\n",
      "Epoch 73/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.8815 - val_loss: 0.1853 - val_accuracy: 0.8997\n",
      "Epoch 74/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.8886 - val_loss: 0.2449 - val_accuracy: 0.8567\n",
      "Epoch 75/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.8922 - val_loss: 0.2032 - val_accuracy: 0.8567\n",
      "Epoch 76/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.8908 - val_loss: 0.1923 - val_accuracy: 0.8567\n",
      "Epoch 77/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.8865 - val_loss: 0.2046 - val_accuracy: 0.8625\n",
      "Epoch 78/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.8800 - val_loss: 0.1851 - val_accuracy: 0.8883\n",
      "Epoch 79/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.8815 - val_loss: 0.1807 - val_accuracy: 0.8567\n",
      "Epoch 80/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.8865 - val_loss: 0.2068 - val_accuracy: 0.8625\n",
      "Epoch 81/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.8879 - val_loss: 0.1785 - val_accuracy: 0.8940\n",
      "Epoch 82/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.8894 - val_loss: 0.1811 - val_accuracy: 0.8510\n",
      "Epoch 83/150\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1686 - accuracy: 0.8807 - val_loss: 0.1828 - val_accuracy: 0.8625\n",
      "Epoch 84/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.8922 - val_loss: 0.1931 - val_accuracy: 0.8911\n",
      "Epoch 85/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.8879 - val_loss: 0.1984 - val_accuracy: 0.8567\n",
      "Epoch 86/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.8901 - val_loss: 0.1823 - val_accuracy: 0.8539\n",
      "Epoch 87/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.8879 - val_loss: 0.1860 - val_accuracy: 0.8510\n",
      "Epoch 88/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.8908 - val_loss: 0.1824 - val_accuracy: 0.8625\n",
      "Epoch 89/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.8944 - val_loss: 0.1761 - val_accuracy: 0.8711\n",
      "Epoch 90/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.8930 - val_loss: 0.1757 - val_accuracy: 0.8682\n",
      "Epoch 91/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.8865 - val_loss: 0.1770 - val_accuracy: 0.8539\n",
      "Epoch 92/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.8807 - val_loss: 0.1805 - val_accuracy: 0.8625\n",
      "Epoch 93/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.8937 - val_loss: 0.1903 - val_accuracy: 0.8625\n",
      "Epoch 94/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.8966 - val_loss: 0.1910 - val_accuracy: 0.8510\n",
      "Epoch 95/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.8815 - val_loss: 0.2083 - val_accuracy: 0.8625\n",
      "Epoch 96/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.8858 - val_loss: 0.1816 - val_accuracy: 0.8539\n",
      "Epoch 97/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.8879 - val_loss: 0.1894 - val_accuracy: 0.8625\n",
      "Epoch 98/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.8851 - val_loss: 0.1769 - val_accuracy: 0.8539\n",
      "Epoch 99/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.8901 - val_loss: 0.1735 - val_accuracy: 0.8968\n",
      "Epoch 100/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.8886 - val_loss: 0.1791 - val_accuracy: 0.8596\n",
      "Epoch 101/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.8930 - val_loss: 0.1715 - val_accuracy: 0.8997\n",
      "Epoch 102/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.8930 - val_loss: 0.1747 - val_accuracy: 0.8596\n",
      "Epoch 103/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.8851 - val_loss: 0.1705 - val_accuracy: 0.8567\n",
      "Epoch 104/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.8843 - val_loss: 0.1927 - val_accuracy: 0.8625\n",
      "Epoch 105/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.8815 - val_loss: 0.1897 - val_accuracy: 0.8539\n",
      "Epoch 106/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.8872 - val_loss: 0.1782 - val_accuracy: 0.8739\n",
      "Epoch 107/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.8858 - val_loss: 0.2121 - val_accuracy: 0.8567\n",
      "Epoch 108/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.8901 - val_loss: 0.1773 - val_accuracy: 0.8539\n",
      "Epoch 109/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.8815 - val_loss: 0.1744 - val_accuracy: 0.8682\n",
      "Epoch 110/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.8858 - val_loss: 0.1708 - val_accuracy: 0.8739\n",
      "Epoch 111/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.8894 - val_loss: 0.1842 - val_accuracy: 0.8968\n",
      "Epoch 112/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.8506 - val_loss: 0.9088 - val_accuracy: 0.8023\n",
      "Epoch 113/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3564 - accuracy: 0.8520 - val_loss: 0.2000 - val_accuracy: 0.8711\n",
      "Epoch 114/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.8836 - val_loss: 0.1886 - val_accuracy: 0.8625\n",
      "Epoch 115/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.8951 - val_loss: 0.2506 - val_accuracy: 0.8625\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.8879 - val_loss: 0.1907 - val_accuracy: 0.8625\n",
      "Epoch 117/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.8894 - val_loss: 0.2272 - val_accuracy: 0.8625\n",
      "Epoch 118/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.8786 - val_loss: 0.1829 - val_accuracy: 0.8625\n",
      "Epoch 119/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.8800 - val_loss: 0.1843 - val_accuracy: 0.8625\n",
      "Epoch 120/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.8671 - val_loss: 0.1747 - val_accuracy: 0.8883\n",
      "Epoch 121/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.8886 - val_loss: 0.2754 - val_accuracy: 0.8395\n",
      "Epoch 122/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.8728 - val_loss: 0.2540 - val_accuracy: 0.8510\n",
      "Epoch 123/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.8772 - val_loss: 0.2488 - val_accuracy: 0.8711\n",
      "Epoch 124/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.8822 - val_loss: 0.1950 - val_accuracy: 0.8625\n",
      "Epoch 125/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.8865 - val_loss: 0.1794 - val_accuracy: 0.8567\n",
      "Epoch 126/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.8973 - val_loss: 0.1803 - val_accuracy: 0.8825\n",
      "Epoch 127/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.8786 - val_loss: 0.1767 - val_accuracy: 0.8625\n",
      "Epoch 128/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.8815 - val_loss: 0.1782 - val_accuracy: 0.8596\n",
      "Epoch 129/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.8879 - val_loss: 0.1915 - val_accuracy: 0.8625\n",
      "Epoch 130/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.8922 - val_loss: 0.2257 - val_accuracy: 0.8625\n",
      "Epoch 131/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.8779 - val_loss: 0.2813 - val_accuracy: 0.8596\n",
      "Epoch 132/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.8894 - val_loss: 0.1825 - val_accuracy: 0.8653\n",
      "Epoch 133/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.8851 - val_loss: 0.1776 - val_accuracy: 0.8567\n",
      "Epoch 134/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.8779 - val_loss: 0.1776 - val_accuracy: 0.8711\n",
      "Epoch 135/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.8779 - val_loss: 0.1782 - val_accuracy: 0.8940\n",
      "Epoch 136/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.8815 - val_loss: 0.1718 - val_accuracy: 0.8567\n",
      "Epoch 137/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.8822 - val_loss: 0.1963 - val_accuracy: 0.8625\n",
      "Epoch 138/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7676 - accuracy: 0.7931 - val_loss: 0.5656 - val_accuracy: 0.8052\n",
      "Epoch 139/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.8678 - val_loss: 0.2520 - val_accuracy: 0.8481\n",
      "Epoch 140/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8678 - val_loss: 0.5280 - val_accuracy: 0.7479\n",
      "Epoch 141/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.8269 - val_loss: 0.2901 - val_accuracy: 0.8481\n",
      "Epoch 142/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.8779 - val_loss: 0.1849 - val_accuracy: 0.8653\n",
      "Epoch 143/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.8807 - val_loss: 0.1745 - val_accuracy: 0.8682\n",
      "Epoch 144/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.8800 - val_loss: 0.1742 - val_accuracy: 0.8653\n",
      "Epoch 145/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.8843 - val_loss: 0.2248 - val_accuracy: 0.8653\n",
      "Epoch 146/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.8800 - val_loss: 0.1692 - val_accuracy: 0.8997\n",
      "Epoch 147/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.8743 - val_loss: 0.1838 - val_accuracy: 0.8653\n",
      "Epoch 148/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.8879 - val_loss: 0.1809 - val_accuracy: 0.8854\n",
      "Epoch 149/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.8843 - val_loss: 0.1990 - val_accuracy: 0.8625\n",
      "Epoch 150/150\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.8858 - val_loss: 0.1695 - val_accuracy: 0.8997\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test' are your datasets\n",
    "num_classes = 8\n",
    "\n",
    "# Concatenate X_train, X_val, and X_test vertically\n",
    "X_combined = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "\n",
    "# Concatenate y_train, y_val, and y_test vertically\n",
    "y_combined = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "# Encode labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_combined_encoded = label_encoder.fit_transform(y_combined)\n",
    "\n",
    "\n",
    "# Define the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Flatten(input_shape=(X_combined.shape[1:])),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_combined, y_combined_encoded, epochs=150, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss, test_accuracy = model.evaluate(X_test_combined, y_test_combined)\n",
    "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e43c5f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'history' is the history object obtained from training your ANN\n",
    "# history = model.fit(...)\n",
    "\n",
    "# Extracting loss values from the history\n",
    "loss_values = history.history['loss']\n",
    "# If you have validation data, you can also extract validation loss values\n",
    "# val_loss_values = history.history['val_loss']\n",
    "\n",
    "# Plotting the loss curve\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "# If you have validation data, you can also plot validation loss\n",
    "# plt.plot(val_loss_values, label='Validation Loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()  # Show legend if you have multiple curves\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52e305d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural_new_aims.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Save the Model\n",
    "\n",
    "import joblib\n",
    "joblib.dump(model, 'neural_new_aims.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34047505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-01 19:38:58,026] A new study created in memory with name: no-name-8b440239-5856-4612-858d-7a8b5f631d75\n",
      "[I 2024-02-01 19:38:59,300] Trial 0 finished with value: 0.5862069129943848 and parameters: {'num_hidden_layers': 1, 'num_neurons': 185, 'dropout_rate': 0.37665738334684395, 'learning_rate': 0.008718849528928816}. Best is trial 0 with value: 0.5862069129943848.\n",
      "[I 2024-02-01 19:39:00,907] Trial 1 finished with value: 0.8620689511299133 and parameters: {'num_hidden_layers': 2, 'num_neurons': 232, 'dropout_rate': 0.370450363954972, 'learning_rate': 0.0011594270167994984}. Best is trial 1 with value: 0.8620689511299133.\n",
      "[I 2024-02-01 19:39:02,189] Trial 2 finished with value: 0.8620689511299133 and parameters: {'num_hidden_layers': 1, 'num_neurons': 241, 'dropout_rate': 0.27679986594002925, 'learning_rate': 0.0018228307051000828}. Best is trial 1 with value: 0.8620689511299133.\n",
      "[I 2024-02-01 19:39:03,634] Trial 3 finished with value: 0.8908045887947083 and parameters: {'num_hidden_layers': 2, 'num_neurons': 127, 'dropout_rate': 0.21846532473725003, 'learning_rate': 0.0007918166821959699}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:05,679] Trial 4 finished with value: 0.8448275923728943 and parameters: {'num_hidden_layers': 3, 'num_neurons': 172, 'dropout_rate': 0.24545944236888964, 'learning_rate': 0.004110220230531594}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:07,536] Trial 5 finished with value: 0.5747126340866089 and parameters: {'num_hidden_layers': 3, 'num_neurons': 197, 'dropout_rate': 0.28030338053742393, 'learning_rate': 0.008298788859385894}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:08,868] Trial 6 finished with value: 0.1149425283074379 and parameters: {'num_hidden_layers': 2, 'num_neurons': 65, 'dropout_rate': 0.29554945924854903, 'learning_rate': 0.009331727951690004}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:10,830] Trial 7 finished with value: 0.8218390941619873 and parameters: {'num_hidden_layers': 3, 'num_neurons': 236, 'dropout_rate': 0.35816215722196404, 'learning_rate': 0.005082852518038326}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:12,303] Trial 8 finished with value: 0.10919540375471115 and parameters: {'num_hidden_layers': 2, 'num_neurons': 67, 'dropout_rate': 0.39413782456767915, 'learning_rate': 0.009720528678981955}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:13,776] Trial 9 finished with value: 0.8620689511299133 and parameters: {'num_hidden_layers': 2, 'num_neurons': 100, 'dropout_rate': 0.2206050674563887, 'learning_rate': 0.0005500263642665191}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:15,024] Trial 10 finished with value: 0.7758620977401733 and parameters: {'num_hidden_layers': 1, 'num_neurons': 130, 'dropout_rate': 0.48069200035081894, 'learning_rate': 0.0035300039944170204}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:16,515] Trial 11 finished with value: 0.8333333134651184 and parameters: {'num_hidden_layers': 2, 'num_neurons': 143, 'dropout_rate': 0.4423748530924592, 'learning_rate': 0.0002086328310795712}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:18,461] Trial 12 finished with value: 0.8103448152542114 and parameters: {'num_hidden_layers': 2, 'num_neurons': 215, 'dropout_rate': 0.3069977806229107, 'learning_rate': 0.0021999178055925285}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:20,115] Trial 13 finished with value: 0.10919540375471115 and parameters: {'num_hidden_layers': 2, 'num_neurons': 116, 'dropout_rate': 0.42094886687275457, 'learning_rate': 0.006345682205399548}. Best is trial 3 with value: 0.8908045887947083.\n",
      "[I 2024-02-01 19:39:21,351] Trial 14 finished with value: 0.9022988677024841 and parameters: {'num_hidden_layers': 1, 'num_neurons': 160, 'dropout_rate': 0.20062037184720355, 'learning_rate': 0.002043934964107098}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:22,580] Trial 15 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 1, 'num_neurons': 156, 'dropout_rate': 0.2039108078688864, 'learning_rate': 0.002825846382051361}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:23,797] Trial 16 finished with value: 0.8275862336158752 and parameters: {'num_hidden_layers': 1, 'num_neurons': 99, 'dropout_rate': 0.2414434629927706, 'learning_rate': 0.005849932408837022}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:25,050] Trial 17 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 1, 'num_neurons': 159, 'dropout_rate': 0.33048027535743413, 'learning_rate': 0.003036975910490896}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:26,764] Trial 18 finished with value: 0.8678160905838013 and parameters: {'num_hidden_layers': 3, 'num_neurons': 130, 'dropout_rate': 0.2518933182872278, 'learning_rate': 0.0013980105357019573}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:28,033] Trial 19 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 1, 'num_neurons': 197, 'dropout_rate': 0.21389707997111176, 'learning_rate': 0.004496130699918862}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:29,674] Trial 20 finished with value: 0.8045976758003235 and parameters: {'num_hidden_layers': 3, 'num_neurons': 95, 'dropout_rate': 0.33450834955964215, 'learning_rate': 0.0004057993269342892}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:32,139] Trial 21 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 3, 'num_neurons': 131, 'dropout_rate': 0.25241373018102936, 'learning_rate': 0.0022609420983949378}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:33,873] Trial 22 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 3, 'num_neurons': 140, 'dropout_rate': 0.20381291577367144, 'learning_rate': 0.0017092203597626327}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:35,741] Trial 23 finished with value: 0.8448275923728943 and parameters: {'num_hidden_layers': 3, 'num_neurons': 114, 'dropout_rate': 0.23961177590264676, 'learning_rate': 0.0010466055671242611}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:37,375] Trial 24 finished with value: 0.8505747318267822 and parameters: {'num_hidden_layers': 2, 'num_neurons': 174, 'dropout_rate': 0.2764152203466735, 'learning_rate': 0.0010938363372728402}. Best is trial 14 with value: 0.9022988677024841.\n",
      "[I 2024-02-01 19:39:38,827] Trial 25 finished with value: 0.9137930870056152 and parameters: {'num_hidden_layers': 2, 'num_neurons': 148, 'dropout_rate': 0.22969630719926004, 'learning_rate': 0.0029825433840342287}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:40,341] Trial 26 finished with value: 0.7873563170433044 and parameters: {'num_hidden_layers': 2, 'num_neurons': 154, 'dropout_rate': 0.23344441810737437, 'learning_rate': 0.002823940160711115}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:41,614] Trial 27 finished with value: 0.8620689511299133 and parameters: {'num_hidden_layers': 1, 'num_neurons': 256, 'dropout_rate': 0.26158187373667174, 'learning_rate': 0.0038845606626185724}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:43,030] Trial 28 finished with value: 0.2586206793785095 and parameters: {'num_hidden_layers': 2, 'num_neurons': 81, 'dropout_rate': 0.22490797987437347, 'learning_rate': 0.004884739121376637}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:44,276] Trial 29 finished with value: 0.5919540524482727 and parameters: {'num_hidden_layers': 1, 'num_neurons': 179, 'dropout_rate': 0.20149794627262435, 'learning_rate': 0.00695726104138774}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:46,213] Trial 30 finished with value: 0.8735632300376892 and parameters: {'num_hidden_layers': 2, 'num_neurons': 194, 'dropout_rate': 0.31119776143464545, 'learning_rate': 0.002430629171069212}. Best is trial 25 with value: 0.9137930870056152.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-01 19:39:47,783] Trial 31 finished with value: 0.7068965435028076 and parameters: {'num_hidden_layers': 2, 'num_neurons': 208, 'dropout_rate': 0.3160390784491476, 'learning_rate': 0.0023558116740007664}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:49,371] Trial 32 finished with value: 0.7816091775894165 and parameters: {'num_hidden_layers': 2, 'num_neurons': 189, 'dropout_rate': 0.2695678798966851, 'learning_rate': 0.003489150451703905}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:50,878] Trial 33 finished with value: 0.8505747318267822 and parameters: {'num_hidden_layers': 2, 'num_neurons': 146, 'dropout_rate': 0.2268652295623537, 'learning_rate': 0.0018989885081735567}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:52,424] Trial 34 finished with value: 0.8390804529190063 and parameters: {'num_hidden_layers': 2, 'num_neurons': 167, 'dropout_rate': 0.29629561277795513, 'learning_rate': 0.000848899661174251}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:54,174] Trial 35 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 2, 'num_neurons': 215, 'dropout_rate': 0.2873241232763339, 'learning_rate': 0.0030457933994480083}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:55,442] Trial 36 finished with value: 0.8390804529190063 and parameters: {'num_hidden_layers': 1, 'num_neurons': 187, 'dropout_rate': 0.3792145611758355, 'learning_rate': 0.0001220312212480651}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:56,921] Trial 37 finished with value: 0.8793103694915771 and parameters: {'num_hidden_layers': 2, 'num_neurons': 118, 'dropout_rate': 0.2585139636069733, 'learning_rate': 0.0015865595341457068}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:58,368] Trial 38 finished with value: 0.8390804529190063 and parameters: {'num_hidden_layers': 2, 'num_neurons': 119, 'dropout_rate': 0.261424514554948, 'learning_rate': 0.001814812203179169}. Best is trial 25 with value: 0.9137930870056152.\n",
      "[I 2024-02-01 19:39:59,566] Trial 39 finished with value: 0.9252873659133911 and parameters: {'num_hidden_layers': 1, 'num_neurons': 124, 'dropout_rate': 0.21725616815509602, 'learning_rate': 0.0015129627004873882}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:00,785] Trial 40 finished with value: 0.8735632300376892 and parameters: {'num_hidden_layers': 1, 'num_neurons': 164, 'dropout_rate': 0.21633880859029175, 'learning_rate': 0.0007026726779485611}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:02,288] Trial 41 finished with value: 0.8390804529190063 and parameters: {'num_hidden_layers': 1, 'num_neurons': 122, 'dropout_rate': 0.2357041898234166, 'learning_rate': 0.0013458442451001092}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:03,516] Trial 42 finished with value: 0.8620689511299133 and parameters: {'num_hidden_layers': 1, 'num_neurons': 110, 'dropout_rate': 0.21746822030745172, 'learning_rate': 0.0014465823784545772}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:04,779] Trial 43 finished with value: 0.8448275923728943 and parameters: {'num_hidden_layers': 1, 'num_neurons': 139, 'dropout_rate': 0.2481826471918681, 'learning_rate': 0.008089374254412165}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:06,215] Trial 44 finished with value: 0.8448275923728943 and parameters: {'num_hidden_layers': 2, 'num_neurons': 107, 'dropout_rate': 0.20156503334882817, 'learning_rate': 0.0033865179310453033}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:07,451] Trial 45 finished with value: 0.8333333134651184 and parameters: {'num_hidden_layers': 1, 'num_neurons': 89, 'dropout_rate': 0.23112768362532893, 'learning_rate': 0.004096389427437108}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:09,051] Trial 46 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 2, 'num_neurons': 147, 'dropout_rate': 0.21492314809075228, 'learning_rate': 0.001940145696024188}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:10,737] Trial 47 finished with value: 0.8735632300376892 and parameters: {'num_hidden_layers': 2, 'num_neurons': 128, 'dropout_rate': 0.26400166616679077, 'learning_rate': 0.0006884281297143977}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:12,038] Trial 48 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 1, 'num_neurons': 135, 'dropout_rate': 0.24954707691040365, 'learning_rate': 0.0026294902274202874}. Best is trial 39 with value: 0.9252873659133911.\n",
      "[I 2024-02-01 19:40:13,334] Trial 49 finished with value: 0.8563218116760254 and parameters: {'num_hidden_layers': 1, 'num_neurons': 150, 'dropout_rate': 0.2260105463477966, 'learning_rate': 0.0015132992419220508}. Best is trial 39 with value: 0.9252873659133911.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 0s 668us/step - loss: 151.0721 - accuracy: 0.3469\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 0s 605us/step - loss: 13.8143 - accuracy: 0.7180\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 0s 569us/step - loss: 4.5632 - accuracy: 0.8030\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 0s 578us/step - loss: 1.9703 - accuracy: 0.8150\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 0s 590us/step - loss: 0.9859 - accuracy: 0.8150\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 0s 581us/step - loss: 0.5133 - accuracy: 0.8380\n",
      "Epoch 7/20\n",
      "55/55 [==============================] - 0s 579us/step - loss: 0.5188 - accuracy: 0.8323\n",
      "Epoch 8/20\n",
      "55/55 [==============================] - 0s 560us/step - loss: 0.3637 - accuracy: 0.8558\n",
      "Epoch 9/20\n",
      "55/55 [==============================] - 0s 577us/step - loss: 0.4596 - accuracy: 0.8403\n",
      "Epoch 10/20\n",
      "55/55 [==============================] - 0s 593us/step - loss: 0.3537 - accuracy: 0.8484\n",
      "Epoch 11/20\n",
      "55/55 [==============================] - 0s 589us/step - loss: 0.2842 - accuracy: 0.8530\n",
      "Epoch 12/20\n",
      "55/55 [==============================] - 0s 565us/step - loss: 0.2884 - accuracy: 0.8633\n",
      "Epoch 13/20\n",
      "55/55 [==============================] - 0s 583us/step - loss: 0.3204 - accuracy: 0.8363\n",
      "Epoch 14/20\n",
      "55/55 [==============================] - 0s 587us/step - loss: 0.3177 - accuracy: 0.8489\n",
      "Epoch 15/20\n",
      "55/55 [==============================] - 0s 581us/step - loss: 0.2888 - accuracy: 0.8564\n",
      "Epoch 16/20\n",
      "55/55 [==============================] - 0s 558us/step - loss: 0.2853 - accuracy: 0.8547\n",
      "Epoch 17/20\n",
      "55/55 [==============================] - 0s 576us/step - loss: 0.2883 - accuracy: 0.8530\n",
      "Epoch 18/20\n",
      "55/55 [==============================] - 0s 570us/step - loss: 0.2484 - accuracy: 0.8621\n",
      "Epoch 19/20\n",
      "55/55 [==============================] - 0s 585us/step - loss: 0.2730 - accuracy: 0.8621\n",
      "Epoch 20/20\n",
      "55/55 [==============================] - 0s 559us/step - loss: 0.2752 - accuracy: 0.8621\n",
      "6/6 [==============================] - 0s 864us/step - loss: 0.2065 - accuracy: 0.8514\n",
      "Best Model Test Accuracy: 85.14%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'X_train', 'X_val', 'y_train', 'y_val' are your datasets\n",
    "num_classes = 8\n",
    "\n",
    "# Encode labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    params = {\n",
    "        'num_hidden_layers': trial.suggest_int('num_hidden_layers', 1, 3),\n",
    "        'num_neurons': trial.suggest_int('num_neurons', 64, 256),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.2, 0.5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2),\n",
    "    }\n",
    "\n",
    "    # Build the neural network model\n",
    "    model = build_neural_network(params)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=params['learning_rate']),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train_encoded, epochs=20, validation_data=(X_val, y_val_encoded), verbose=0)\n",
    "\n",
    "    # Get the validation accuracy\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "# Define a function to build the neural network model\n",
    "def build_neural_network(params):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(X_train.shape[1:])))\n",
    "    \n",
    "    # Add hidden layers based on the number specified by 'num_hidden_layers'\n",
    "    for _ in range(params['num_hidden_layers']):\n",
    "        model.add(layers.Dense(params['num_neurons'], activation='relu'))\n",
    "        model.add(layers.Dropout(params['dropout_rate']))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_model = build_neural_network(best_params)\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "best_model.fit(X_combined, y_combined, epochs=20)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Best Model Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e2f4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "55/55 [==============================] - 0s 740us/step - loss: 0.2534 - accuracy: 0.8759\n",
      "Epoch 2/200\n",
      "55/55 [==============================] - 0s 678us/step - loss: 0.2464 - accuracy: 0.8639\n",
      "Epoch 3/200\n",
      "55/55 [==============================] - 0s 717us/step - loss: 0.2389 - accuracy: 0.8782\n",
      "Epoch 4/200\n",
      "55/55 [==============================] - 0s 936us/step - loss: 0.2224 - accuracy: 0.8713\n",
      "Epoch 5/200\n",
      "55/55 [==============================] - 0s 653us/step - loss: 0.2256 - accuracy: 0.8667\n",
      "Epoch 6/200\n",
      "55/55 [==============================] - 0s 694us/step - loss: 0.2172 - accuracy: 0.8719\n",
      "Epoch 7/200\n",
      "55/55 [==============================] - 0s 660us/step - loss: 0.2291 - accuracy: 0.8581\n",
      "Epoch 8/200\n",
      "55/55 [==============================] - 0s 627us/step - loss: 0.2418 - accuracy: 0.8627\n",
      "Epoch 9/200\n",
      "55/55 [==============================] - 0s 666us/step - loss: 0.2559 - accuracy: 0.8679\n",
      "Epoch 10/200\n",
      "55/55 [==============================] - 0s 647us/step - loss: 0.2343 - accuracy: 0.8696\n",
      "Epoch 11/200\n",
      "55/55 [==============================] - 0s 637us/step - loss: 0.2655 - accuracy: 0.8610\n",
      "Epoch 12/200\n",
      "55/55 [==============================] - 0s 648us/step - loss: 0.2486 - accuracy: 0.8541\n",
      "Epoch 13/200\n",
      "55/55 [==============================] - 0s 640us/step - loss: 0.2372 - accuracy: 0.8696\n",
      "Epoch 14/200\n",
      "55/55 [==============================] - 0s 641us/step - loss: 0.2464 - accuracy: 0.8656\n",
      "Epoch 15/200\n",
      "55/55 [==============================] - 0s 622us/step - loss: 0.2383 - accuracy: 0.8650\n",
      "Epoch 16/200\n",
      "55/55 [==============================] - 0s 644us/step - loss: 0.2363 - accuracy: 0.8696\n",
      "Epoch 17/200\n",
      "55/55 [==============================] - 0s 638us/step - loss: 0.2358 - accuracy: 0.8541\n",
      "Epoch 18/200\n",
      "55/55 [==============================] - 0s 640us/step - loss: 0.2751 - accuracy: 0.8535\n",
      "Epoch 19/200\n",
      "55/55 [==============================] - 0s 730us/step - loss: 0.2422 - accuracy: 0.8587\n",
      "Epoch 20/200\n",
      "55/55 [==============================] - 0s 778us/step - loss: 0.2327 - accuracy: 0.8679\n",
      "Epoch 21/200\n",
      "55/55 [==============================] - 0s 755us/step - loss: 0.2251 - accuracy: 0.8667\n",
      "Epoch 22/200\n",
      "55/55 [==============================] - 0s 653us/step - loss: 0.2160 - accuracy: 0.8581\n",
      "Epoch 23/200\n",
      "55/55 [==============================] - 0s 649us/step - loss: 0.2412 - accuracy: 0.8530\n",
      "Epoch 24/200\n",
      "55/55 [==============================] - 0s 645us/step - loss: 0.2233 - accuracy: 0.8754\n",
      "Epoch 25/200\n",
      "55/55 [==============================] - 0s 686us/step - loss: 0.2401 - accuracy: 0.8604\n",
      "Epoch 26/200\n",
      "55/55 [==============================] - 0s 680us/step - loss: 0.2353 - accuracy: 0.8599\n",
      "Epoch 27/200\n",
      "55/55 [==============================] - 0s 641us/step - loss: 0.2102 - accuracy: 0.8736\n",
      "Epoch 28/200\n",
      "55/55 [==============================] - 0s 632us/step - loss: 0.2618 - accuracy: 0.8507\n",
      "Epoch 29/200\n",
      "55/55 [==============================] - 0s 659us/step - loss: 0.2710 - accuracy: 0.8512\n",
      "Epoch 30/200\n",
      "55/55 [==============================] - 0s 648us/step - loss: 0.3060 - accuracy: 0.8374\n",
      "Epoch 31/200\n",
      "55/55 [==============================] - 0s 661us/step - loss: 0.2351 - accuracy: 0.8650\n",
      "Epoch 32/200\n",
      "55/55 [==============================] - 0s 649us/step - loss: 0.2365 - accuracy: 0.8599\n",
      "Epoch 33/200\n",
      "55/55 [==============================] - 0s 639us/step - loss: 0.2308 - accuracy: 0.8644\n",
      "Epoch 34/200\n",
      "55/55 [==============================] - 0s 655us/step - loss: 0.2181 - accuracy: 0.8639\n",
      "Epoch 35/200\n",
      "55/55 [==============================] - 0s 617us/step - loss: 0.3060 - accuracy: 0.8576\n",
      "Epoch 36/200\n",
      "55/55 [==============================] - 0s 644us/step - loss: 0.2812 - accuracy: 0.8426\n",
      "Epoch 37/200\n",
      "55/55 [==============================] - 0s 715us/step - loss: 0.2500 - accuracy: 0.8564\n",
      "Epoch 38/200\n",
      "55/55 [==============================] - 0s 719us/step - loss: 0.2219 - accuracy: 0.8690\n",
      "Epoch 39/200\n",
      "55/55 [==============================] - 0s 704us/step - loss: 0.2367 - accuracy: 0.8593\n",
      "Epoch 40/200\n",
      "55/55 [==============================] - 0s 644us/step - loss: 0.2380 - accuracy: 0.8679\n",
      "Epoch 41/200\n",
      "55/55 [==============================] - 0s 650us/step - loss: 0.2519 - accuracy: 0.8570\n",
      "Epoch 42/200\n",
      "55/55 [==============================] - 0s 630us/step - loss: 0.2414 - accuracy: 0.8581\n",
      "Epoch 43/200\n",
      "55/55 [==============================] - 0s 637us/step - loss: 0.2367 - accuracy: 0.8621\n",
      "Epoch 44/200\n",
      "55/55 [==============================] - 0s 627us/step - loss: 0.2143 - accuracy: 0.8800\n",
      "Epoch 45/200\n",
      "55/55 [==============================] - 0s 624us/step - loss: 0.2195 - accuracy: 0.8633\n",
      "Epoch 46/200\n",
      "55/55 [==============================] - 0s 640us/step - loss: 0.2837 - accuracy: 0.8472\n",
      "Epoch 47/200\n",
      "55/55 [==============================] - 0s 622us/step - loss: 0.3083 - accuracy: 0.8363\n",
      "Epoch 48/200\n",
      "55/55 [==============================] - 0s 625us/step - loss: 0.3424 - accuracy: 0.8248\n",
      "Epoch 49/200\n",
      "55/55 [==============================] - 0s 627us/step - loss: 0.3677 - accuracy: 0.8374\n",
      "Epoch 50/200\n",
      "55/55 [==============================] - 0s 645us/step - loss: 0.3125 - accuracy: 0.8369\n",
      "Epoch 51/200\n",
      "55/55 [==============================] - 0s 658us/step - loss: 0.2894 - accuracy: 0.8386\n",
      "Epoch 52/200\n",
      "55/55 [==============================] - 0s 654us/step - loss: 0.2841 - accuracy: 0.8392\n",
      "Epoch 53/200\n",
      "55/55 [==============================] - 0s 659us/step - loss: 0.3427 - accuracy: 0.8219\n",
      "Epoch 54/200\n",
      "55/55 [==============================] - 0s 656us/step - loss: 0.3707 - accuracy: 0.8208\n",
      "Epoch 55/200\n",
      "55/55 [==============================] - 0s 717us/step - loss: 0.2933 - accuracy: 0.8449\n",
      "Epoch 56/200\n",
      "55/55 [==============================] - 0s 701us/step - loss: 0.2619 - accuracy: 0.8616\n",
      "Epoch 57/200\n",
      "55/55 [==============================] - 0s 641us/step - loss: 0.2826 - accuracy: 0.8346\n",
      "Epoch 58/200\n",
      "55/55 [==============================] - 0s 629us/step - loss: 0.2556 - accuracy: 0.8518\n",
      "Epoch 59/200\n",
      "55/55 [==============================] - 0s 640us/step - loss: 0.2379 - accuracy: 0.8599\n",
      "Epoch 60/200\n",
      "55/55 [==============================] - 0s 679us/step - loss: 0.2179 - accuracy: 0.8685\n",
      "Epoch 61/200\n",
      "55/55 [==============================] - 0s 644us/step - loss: 0.2496 - accuracy: 0.8633\n",
      "Epoch 62/200\n",
      "55/55 [==============================] - 0s 614us/step - loss: 0.2155 - accuracy: 0.8748\n",
      "Epoch 63/200\n",
      "55/55 [==============================] - 0s 635us/step - loss: 0.2358 - accuracy: 0.8627\n",
      "Epoch 64/200\n",
      "55/55 [==============================] - 0s 633us/step - loss: 0.3125 - accuracy: 0.8443\n",
      "Epoch 65/200\n",
      "55/55 [==============================] - 0s 843us/step - loss: 0.2379 - accuracy: 0.8472\n",
      "Epoch 66/200\n",
      "55/55 [==============================] - 0s 617us/step - loss: 0.2923 - accuracy: 0.8449\n",
      "Epoch 67/200\n",
      "55/55 [==============================] - 0s 645us/step - loss: 0.2846 - accuracy: 0.8432\n",
      "Epoch 68/200\n",
      "55/55 [==============================] - 0s 634us/step - loss: 0.2875 - accuracy: 0.8478\n",
      "Epoch 69/200\n",
      "55/55 [==============================] - 0s 607us/step - loss: 0.2278 - accuracy: 0.8696\n",
      "Epoch 70/200\n",
      "55/55 [==============================] - 0s 613us/step - loss: 0.2546 - accuracy: 0.8501\n",
      "Epoch 71/200\n",
      "55/55 [==============================] - 0s 641us/step - loss: 0.2342 - accuracy: 0.8736\n",
      "Epoch 72/200\n",
      "55/55 [==============================] - 0s 728us/step - loss: 0.2695 - accuracy: 0.8455\n",
      "Epoch 73/200\n",
      "55/55 [==============================] - 0s 707us/step - loss: 0.3973 - accuracy: 0.8242\n",
      "Epoch 74/200\n",
      "55/55 [==============================] - 0s 630us/step - loss: 0.2630 - accuracy: 0.8541\n",
      "Epoch 75/200\n",
      "55/55 [==============================] - 0s 643us/step - loss: 0.2761 - accuracy: 0.8610\n",
      "Epoch 76/200\n",
      "55/55 [==============================] - 0s 635us/step - loss: 0.3770 - accuracy: 0.8202\n",
      "Epoch 77/200\n",
      "55/55 [==============================] - 0s 640us/step - loss: 0.4023 - accuracy: 0.8099\n",
      "Epoch 78/200\n",
      "55/55 [==============================] - 0s 652us/step - loss: 0.3610 - accuracy: 0.8093\n",
      "Epoch 79/200\n",
      "55/55 [==============================] - 0s 631us/step - loss: 0.3083 - accuracy: 0.8323\n",
      "Epoch 80/200\n",
      "55/55 [==============================] - 0s 628us/step - loss: 0.2893 - accuracy: 0.8478\n",
      "Epoch 81/200\n",
      "55/55 [==============================] - 0s 631us/step - loss: 0.3237 - accuracy: 0.8265\n",
      "Epoch 82/200\n",
      "55/55 [==============================] - 0s 624us/step - loss: 0.2711 - accuracy: 0.8524\n",
      "Epoch 83/200\n",
      "55/55 [==============================] - 0s 622us/step - loss: 0.2586 - accuracy: 0.8553\n",
      "Epoch 84/200\n",
      "55/55 [==============================] - 0s 599us/step - loss: 0.2421 - accuracy: 0.8518\n",
      "Epoch 85/200\n",
      "55/55 [==============================] - 0s 598us/step - loss: 0.2910 - accuracy: 0.8507\n",
      "Epoch 86/200\n",
      "55/55 [==============================] - 0s 604us/step - loss: 0.3330 - accuracy: 0.8242\n",
      "Epoch 87/200\n",
      "55/55 [==============================] - 0s 599us/step - loss: 0.3091 - accuracy: 0.8420\n",
      "Epoch 88/200\n",
      "55/55 [==============================] - 0s 596us/step - loss: 0.3493 - accuracy: 0.8116\n",
      "Epoch 89/200\n",
      "55/55 [==============================] - 0s 616us/step - loss: 0.3650 - accuracy: 0.8041\n",
      "Epoch 90/200\n",
      "55/55 [==============================] - 0s 664us/step - loss: 0.3585 - accuracy: 0.8145\n",
      "Epoch 91/200\n",
      "55/55 [==============================] - 0s 678us/step - loss: 0.2981 - accuracy: 0.8323\n",
      "Epoch 92/200\n",
      "55/55 [==============================] - 0s 636us/step - loss: 0.3505 - accuracy: 0.8237\n",
      "Epoch 93/200\n",
      "55/55 [==============================] - 0s 599us/step - loss: 0.3725 - accuracy: 0.8070\n",
      "Epoch 94/200\n",
      "55/55 [==============================] - 0s 612us/step - loss: 0.4067 - accuracy: 0.7990\n",
      "Epoch 95/200\n",
      "55/55 [==============================] - 0s 625us/step - loss: 0.4882 - accuracy: 0.7771\n",
      "Epoch 96/200\n",
      "55/55 [==============================] - 0s 619us/step - loss: 0.4263 - accuracy: 0.7783\n",
      "Epoch 97/200\n",
      "55/55 [==============================] - 0s 617us/step - loss: 0.3751 - accuracy: 0.7995\n",
      "Epoch 98/200\n",
      "55/55 [==============================] - 0s 597us/step - loss: 0.3497 - accuracy: 0.7961\n",
      "Epoch 99/200\n",
      "55/55 [==============================] - 0s 606us/step - loss: 0.3228 - accuracy: 0.8237\n",
      "Epoch 100/200\n",
      "55/55 [==============================] - 0s 606us/step - loss: 0.3264 - accuracy: 0.8219\n",
      "Epoch 101/200\n",
      "55/55 [==============================] - 0s 608us/step - loss: 0.3554 - accuracy: 0.8145\n",
      "Epoch 102/200\n",
      "55/55 [==============================] - 0s 609us/step - loss: 0.3815 - accuracy: 0.7984\n",
      "Epoch 103/200\n",
      "55/55 [==============================] - 0s 622us/step - loss: 0.3574 - accuracy: 0.8196\n",
      "Epoch 104/200\n",
      "55/55 [==============================] - 0s 618us/step - loss: 0.3471 - accuracy: 0.8191\n",
      "Epoch 105/200\n",
      "55/55 [==============================] - 0s 607us/step - loss: 0.3988 - accuracy: 0.7938\n",
      "Epoch 106/200\n",
      "55/55 [==============================] - 0s 621us/step - loss: 0.4695 - accuracy: 0.7662\n",
      "Epoch 107/200\n",
      "55/55 [==============================] - 0s 608us/step - loss: 0.3831 - accuracy: 0.8001\n",
      "Epoch 108/200\n",
      "55/55 [==============================] - 0s 684us/step - loss: 0.3184 - accuracy: 0.8334\n",
      "Epoch 109/200\n",
      "55/55 [==============================] - 0s 677us/step - loss: 0.3452 - accuracy: 0.8208\n",
      "Epoch 110/200\n",
      "55/55 [==============================] - 0s 621us/step - loss: 0.3035 - accuracy: 0.8374\n",
      "Epoch 111/200\n",
      "55/55 [==============================] - 0s 619us/step - loss: 0.3436 - accuracy: 0.8271\n",
      "Epoch 112/200\n",
      "55/55 [==============================] - 0s 614us/step - loss: 0.3295 - accuracy: 0.8208\n",
      "Epoch 113/200\n",
      "55/55 [==============================] - 0s 621us/step - loss: 0.3260 - accuracy: 0.8271\n",
      "Epoch 114/200\n",
      "55/55 [==============================] - 0s 608us/step - loss: 0.3031 - accuracy: 0.8294\n",
      "Epoch 115/200\n",
      "55/55 [==============================] - 0s 614us/step - loss: 0.3231 - accuracy: 0.8237\n",
      "Epoch 116/200\n",
      "55/55 [==============================] - 0s 615us/step - loss: 0.3490 - accuracy: 0.8277\n",
      "Epoch 117/200\n",
      "55/55 [==============================] - 0s 626us/step - loss: 0.2995 - accuracy: 0.8472\n",
      "Epoch 118/200\n",
      "55/55 [==============================] - 0s 626us/step - loss: 0.4120 - accuracy: 0.8128\n",
      "Epoch 119/200\n",
      "55/55 [==============================] - 0s 605us/step - loss: 0.3944 - accuracy: 0.8024\n",
      "Epoch 120/200\n",
      "55/55 [==============================] - 0s 621us/step - loss: 0.3242 - accuracy: 0.8311\n",
      "Epoch 121/200\n",
      "55/55 [==============================] - 0s 620us/step - loss: 0.3452 - accuracy: 0.8191\n",
      "Epoch 122/200\n",
      "55/55 [==============================] - 0s 611us/step - loss: 0.3365 - accuracy: 0.8329\n",
      "Epoch 123/200\n",
      "55/55 [==============================] - 0s 624us/step - loss: 0.3032 - accuracy: 0.8484\n",
      "Epoch 124/200\n",
      "55/55 [==============================] - 0s 638us/step - loss: 0.3136 - accuracy: 0.8380\n",
      "Epoch 125/200\n",
      "55/55 [==============================] - 0s 677us/step - loss: 0.3379 - accuracy: 0.8294\n",
      "Epoch 126/200\n",
      "55/55 [==============================] - 0s 660us/step - loss: 0.3320 - accuracy: 0.8288\n",
      "Epoch 127/200\n",
      "55/55 [==============================] - 0s 630us/step - loss: 0.3157 - accuracy: 0.8380\n",
      "Epoch 128/200\n",
      "55/55 [==============================] - 0s 644us/step - loss: 0.2959 - accuracy: 0.8392\n",
      "Epoch 129/200\n",
      "55/55 [==============================] - 0s 626us/step - loss: 0.3150 - accuracy: 0.8380\n",
      "Epoch 130/200\n",
      "55/55 [==============================] - 0s 616us/step - loss: 0.3666 - accuracy: 0.8156\n",
      "Epoch 131/200\n",
      "55/55 [==============================] - 0s 616us/step - loss: 0.3577 - accuracy: 0.8237\n",
      "Epoch 132/200\n",
      "55/55 [==============================] - 0s 814us/step - loss: 0.3238 - accuracy: 0.8415\n",
      "Epoch 133/200\n",
      "55/55 [==============================] - 0s 618us/step - loss: 0.2956 - accuracy: 0.8392\n",
      "Epoch 134/200\n",
      "55/55 [==============================] - 0s 613us/step - loss: 0.3542 - accuracy: 0.8219\n",
      "Epoch 135/200\n",
      "55/55 [==============================] - 0s 607us/step - loss: 0.3104 - accuracy: 0.8420\n",
      "Epoch 136/200\n",
      "55/55 [==============================] - 0s 613us/step - loss: 0.3159 - accuracy: 0.8374\n",
      "Epoch 137/200\n",
      "55/55 [==============================] - 0s 604us/step - loss: 0.3587 - accuracy: 0.8300\n",
      "Epoch 138/200\n",
      "55/55 [==============================] - 0s 620us/step - loss: 0.3659 - accuracy: 0.8208\n",
      "Epoch 139/200\n",
      "55/55 [==============================] - 0s 625us/step - loss: 0.3693 - accuracy: 0.8202\n",
      "Epoch 140/200\n",
      "55/55 [==============================] - 0s 628us/step - loss: 0.4393 - accuracy: 0.7926\n",
      "Epoch 141/200\n",
      "55/55 [==============================] - 0s 624us/step - loss: 0.3833 - accuracy: 0.8110\n",
      "Epoch 142/200\n",
      "55/55 [==============================] - 0s 641us/step - loss: 0.3705 - accuracy: 0.8041\n",
      "Epoch 143/200\n",
      "55/55 [==============================] - 0s 652us/step - loss: 0.3471 - accuracy: 0.8346\n",
      "Epoch 144/200\n",
      "55/55 [==============================] - 0s 630us/step - loss: 0.3572 - accuracy: 0.8214\n",
      "Epoch 145/200\n",
      "55/55 [==============================] - 0s 628us/step - loss: 0.3901 - accuracy: 0.8122\n",
      "Epoch 146/200\n",
      "55/55 [==============================] - 0s 632us/step - loss: 0.3651 - accuracy: 0.8128\n",
      "Epoch 147/200\n",
      "55/55 [==============================] - 0s 622us/step - loss: 0.3628 - accuracy: 0.8173\n",
      "Epoch 148/200\n",
      "55/55 [==============================] - 0s 613us/step - loss: 0.3120 - accuracy: 0.8363\n",
      "Epoch 149/200\n",
      "55/55 [==============================] - 0s 612us/step - loss: 0.3510 - accuracy: 0.8162\n",
      "Epoch 150/200\n",
      "55/55 [==============================] - 0s 632us/step - loss: 0.3487 - accuracy: 0.8196\n",
      "Epoch 151/200\n",
      "55/55 [==============================] - 0s 626us/step - loss: 0.4664 - accuracy: 0.7812\n",
      "Epoch 152/200\n",
      "55/55 [==============================] - 0s 620us/step - loss: 0.4402 - accuracy: 0.7938\n",
      "Epoch 153/200\n",
      "55/55 [==============================] - 0s 615us/step - loss: 0.4224 - accuracy: 0.7915\n",
      "Epoch 154/200\n",
      "55/55 [==============================] - 0s 610us/step - loss: 0.3711 - accuracy: 0.8053\n",
      "Epoch 155/200\n",
      "55/55 [==============================] - 0s 610us/step - loss: 0.3175 - accuracy: 0.8438\n",
      "Epoch 156/200\n",
      "55/55 [==============================] - 0s 616us/step - loss: 0.3035 - accuracy: 0.8541\n",
      "Epoch 157/200\n",
      "55/55 [==============================] - 0s 652us/step - loss: 0.3794 - accuracy: 0.8156\n",
      "Epoch 158/200\n",
      "55/55 [==============================] - 0s 622us/step - loss: 0.3629 - accuracy: 0.8173\n",
      "Epoch 159/200\n",
      "55/55 [==============================] - 0s 677us/step - loss: 0.3780 - accuracy: 0.8064\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 690us/step - loss: 0.3733 - accuracy: 0.8179\n",
      "Epoch 161/200\n",
      "55/55 [==============================] - 0s 663us/step - loss: 0.3615 - accuracy: 0.8145\n",
      "Epoch 162/200\n",
      "55/55 [==============================] - 0s 674us/step - loss: 0.3370 - accuracy: 0.8237\n",
      "Epoch 163/200\n",
      "55/55 [==============================] - 0s 690us/step - loss: 0.3586 - accuracy: 0.8122\n",
      "Epoch 164/200\n",
      "55/55 [==============================] - 0s 646us/step - loss: 0.3592 - accuracy: 0.8105\n",
      "Epoch 165/200\n",
      "55/55 [==============================] - 0s 605us/step - loss: 0.3519 - accuracy: 0.8196\n",
      "Epoch 166/200\n",
      "55/55 [==============================] - 0s 597us/step - loss: 0.3520 - accuracy: 0.8225\n",
      "Epoch 167/200\n",
      "55/55 [==============================] - 0s 609us/step - loss: 0.3843 - accuracy: 0.7978\n",
      "Epoch 168/200\n",
      "55/55 [==============================] - 0s 625us/step - loss: 0.3579 - accuracy: 0.8018\n",
      "Epoch 169/200\n",
      "55/55 [==============================] - 0s 594us/step - loss: 0.3552 - accuracy: 0.8064\n",
      "Epoch 170/200\n",
      "55/55 [==============================] - 0s 597us/step - loss: 0.3987 - accuracy: 0.8030\n",
      "Epoch 171/200\n",
      "55/55 [==============================] - 0s 607us/step - loss: 0.4117 - accuracy: 0.7904\n",
      "Epoch 172/200\n",
      "55/55 [==============================] - 0s 609us/step - loss: 0.4590 - accuracy: 0.7840\n",
      "Epoch 173/200\n",
      "55/55 [==============================] - 0s 591us/step - loss: 0.3828 - accuracy: 0.8070\n",
      "Epoch 174/200\n",
      "55/55 [==============================] - 0s 595us/step - loss: 0.4065 - accuracy: 0.8053\n",
      "Epoch 175/200\n",
      "55/55 [==============================] - 0s 591us/step - loss: 0.4134 - accuracy: 0.7949\n",
      "Epoch 176/200\n",
      "55/55 [==============================] - 0s 611us/step - loss: 0.3630 - accuracy: 0.8191\n",
      "Epoch 177/200\n",
      "55/55 [==============================] - 0s 620us/step - loss: 0.4047 - accuracy: 0.7835\n",
      "Epoch 178/200\n",
      "55/55 [==============================] - 0s 620us/step - loss: 0.4754 - accuracy: 0.7720\n",
      "Epoch 179/200\n",
      "55/55 [==============================] - 0s 598us/step - loss: 0.4149 - accuracy: 0.7869\n",
      "Epoch 180/200\n",
      "55/55 [==============================] - 0s 685us/step - loss: 0.3832 - accuracy: 0.7898\n",
      "Epoch 181/200\n",
      "55/55 [==============================] - 0s 685us/step - loss: 0.4178 - accuracy: 0.8001\n",
      "Epoch 182/200\n",
      "55/55 [==============================] - 0s 608us/step - loss: 0.4114 - accuracy: 0.7863\n",
      "Epoch 183/200\n",
      "55/55 [==============================] - 0s 605us/step - loss: 0.3757 - accuracy: 0.8087\n",
      "Epoch 184/200\n",
      "55/55 [==============================] - 0s 621us/step - loss: 0.3457 - accuracy: 0.8214\n",
      "Epoch 185/200\n",
      "55/55 [==============================] - 0s 647us/step - loss: 0.4590 - accuracy: 0.7731\n",
      "Epoch 186/200\n",
      "55/55 [==============================] - 0s 612us/step - loss: 0.4250 - accuracy: 0.7846\n",
      "Epoch 187/200\n",
      "55/55 [==============================] - 0s 611us/step - loss: 0.3845 - accuracy: 0.8007\n",
      "Epoch 188/200\n",
      "55/55 [==============================] - 0s 613us/step - loss: 0.3635 - accuracy: 0.8093\n",
      "Epoch 189/200\n",
      "55/55 [==============================] - 0s 611us/step - loss: 0.3528 - accuracy: 0.8133\n",
      "Epoch 190/200\n",
      "55/55 [==============================] - 0s 610us/step - loss: 0.3577 - accuracy: 0.8150\n",
      "Epoch 191/200\n",
      "55/55 [==============================] - 0s 611us/step - loss: 0.3503 - accuracy: 0.8162\n",
      "Epoch 192/200\n",
      "55/55 [==============================] - 0s 623us/step - loss: 0.3368 - accuracy: 0.8173\n",
      "Epoch 193/200\n",
      "55/55 [==============================] - 0s 610us/step - loss: 0.3741 - accuracy: 0.8099\n",
      "Epoch 194/200\n",
      "55/55 [==============================] - 0s 625us/step - loss: 0.3285 - accuracy: 0.8311\n",
      "Epoch 195/200\n",
      "55/55 [==============================] - 0s 629us/step - loss: 0.3159 - accuracy: 0.8288\n",
      "Epoch 196/200\n",
      "55/55 [==============================] - 0s 603us/step - loss: 0.3071 - accuracy: 0.8409\n",
      "Epoch 197/200\n",
      "55/55 [==============================] - 0s 667us/step - loss: 0.3245 - accuracy: 0.8306\n",
      "Epoch 198/200\n",
      "55/55 [==============================] - 0s 668us/step - loss: 0.3195 - accuracy: 0.8208\n",
      "Epoch 199/200\n",
      "55/55 [==============================] - 0s 649us/step - loss: 0.3701 - accuracy: 0.8237\n",
      "Epoch 200/200\n",
      "55/55 [==============================] - 0s 625us/step - loss: 0.4910 - accuracy: 0.7622\n",
      "6/6 [==============================] - 0s 776us/step - loss: 0.2085 - accuracy: 0.8629\n",
      "Best Model Test Accuracy: 86.29%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model.fit(X_combined, y_combined, epochs=200)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Best Model Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccfb83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
